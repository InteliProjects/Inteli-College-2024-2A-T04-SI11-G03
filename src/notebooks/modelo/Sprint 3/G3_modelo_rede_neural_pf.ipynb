{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:48:36.586441Z",
     "start_time": "2024-09-09T12:45:26.555649Z"
    },
    "id": "zVRgHAMEf0AG"
   },
   "outputs": [],
   "source": [
    "# @title downloads e importações {\"display-mode\":\"form\"}\n",
    "\n",
    "! pip install imblearn gdown keras_tuner keras\n",
    "\n",
    "import warnings # type: ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.preprocessing import RobustScaler # type: ignore\n",
    "from imblearn.over_sampling import SMOTE # type: ignore\n",
    "import seaborn as sns # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import gdown # type: ignore\n",
    "from sklearn.cluster import KMeans # type: ignore\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # type: ignore\n",
    "import keras_tuner as kt # type: ignore\n",
    "import tensorflow.keras as keras # type: ignore\n",
    "from tensorflow.keras import layers # type: ignore\n",
    "from mpl_toolkits.mplot3d import Axes3D # type: ignore\n",
    "\n",
    "\n",
    "# ids = {\n",
    "#     \"consumo_2024\": \"1-iXT7eaJWQokHf9cyfrB8N5wvkdhgjJW\",\n",
    "#     \"consumo_2023\": \"1-WfvkRwaRr85B_Joxcm9xVdpyg5NBAmp\",\n",
    "#     \"consumo_2022\": \"1-Uu4Tf4lufJVFeJnYKc5w7OeW66pe1eC\",\n",
    "#     \"consumo_2021\": \"1-2PsTLzG4dcY4wM0p7vFfabUuLv950gC\",\n",
    "#     \"consumo_2020\": \"1-1pOoa0eJlNJ94BMi7p4PTx5KUS96mhX\",\n",
    "#     \"consumo_2019\": \"1-2PsTLzG4dcY4wM0p7vFfabUuLv950gC\",\n",
    "#     \"fraudes\": \"1-MbIlChqQapcxFkoJgpbQIsN9FBLfbX1\"\n",
    "# }\n",
    "\n",
    "ids = {\n",
    "    \"consumo_2023\": \"1-WfvkRwaRr85B_Joxcm9xVdpyg5NBAmp\",\n",
    "    \"fraudes\": \"1-MbIlChqQapcxFkoJgpbQIsN9FBLfbX1\"\n",
    "}\n",
    "\n",
    "for key, file_id in ids.items():\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    arquivo_destino = \"dataset_{}.csv\".format(key)\n",
    "    gdown.download(url, arquivo_destino, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:56:26.084387Z",
     "start_time": "2024-09-09T12:56:26.079105Z"
    },
    "id": "vAw9nhuCf0AI"
   },
   "outputs": [],
   "source": [
    "# @title utils {\"display-mode\":\"form\"}\n",
    "\n",
    "# funcs\n",
    "\n",
    "def reduzir_dataset(df, n=10000):\n",
    "    \"\"\" diminui o dataset, levando em consideração a matrícula\n",
    "    \"\"\"\n",
    "    matriculas = df['MATRICULA'].unique()[:n]\n",
    "    df_filtrado = df[df['MATRICULA'].isin(matriculas)]\n",
    "    return df_filtrado\n",
    "\n",
    "\n",
    "# plot\n",
    "\n",
    "def plotar_clusters(df, n_clusters):\n",
    "    \"\"\"\n",
    "    plota os clusters em um gráfico de dispersão.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(df['COD_LONGITUDE'], df['COD_LATITUDE'], c=df['cluster'], cmap='viridis', marker='o', s=100)\n",
    "    plt.title(f\"K-Means Clustering com {n_clusters} Clusters\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.show()\n",
    "\n",
    "def plotar_box_plot(df_filtrado, column_name, print_text):\n",
    "    \"\"\"\n",
    "    plota um boxplot para uma coluna específica do DataFrame.\n",
    "    \"\"\"\n",
    "    sns.boxplot(x=df_filtrado[column_name])\n",
    "    plt.title(f'Boxplot de {column_name} ({print_text})')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.show()\n",
    "\n",
    "def plotar_matrix_de_consusao(cm):\n",
    "    \"\"\"\n",
    "    plota uma matriz de confusão para avaliar o desempenho do modelo.\n",
    "    \"\"\"\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Matriz de Confusão - Melhor Modelo')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.show()\n",
    "\n",
    "def plotar_hiperparametros_2d():\n",
    "    \"\"\"\n",
    "    plota hiperparametros 2d\n",
    "    \"\"\"\n",
    "    trial_data = []\n",
    "\n",
    "    for trial in tuner.oracle.get_best_trials(num_trials=20):\n",
    "        data = trial.hyperparameters.values.copy()\n",
    "        data['score'] = trial.score\n",
    "\n",
    "        units_keys = [key for key in data.keys() if 'units_' in key]\n",
    "        dropout_keys = [key for key in data.keys() if 'dropout_' in key]\n",
    "\n",
    "        data['units_mean'] = np.mean([data[key] for key in units_keys])\n",
    "        data['dropout_mean'] = np.mean([data[key] for key in dropout_keys])\n",
    "\n",
    "        trial_data.append(data)\n",
    "\n",
    "    df = pd.DataFrame(trial_data)\n",
    "\n",
    "    hp_x = 'learning_rate'\n",
    "    hp_y = 'units_0'\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.scatterplot(data=df, x=hp_x, y=hp_y, size='score', hue='score', palette='viridis', legend='full')\n",
    "    plt.title(f'Interação entre {hp_x} e {hp_y}')\n",
    "    plt.xlabel(hp_x)\n",
    "    plt.ylabel(hp_y)\n",
    "    plt.show()\n",
    "\n",
    "def plotar_hiperparametros_3d(tuner):\n",
    "    \"\"\"\n",
    "    plota hiperparâmetros em um gráfico 3D para análise de resultados.\n",
    "    \"\"\"\n",
    "    trial_data = []\n",
    "\n",
    "    for trial in tuner.oracle.get_best_trials(num_trials=20):\n",
    "        data = trial.hyperparameters.values.copy()\n",
    "        data['score'] = trial.score\n",
    "\n",
    "        units_keys = [key for key in data.keys() if 'units_' in key]\n",
    "        dropout_keys = [key for key in data.keys() if 'dropout_' in key]\n",
    "\n",
    "        data['units_mean'] = np.mean([data[key] for key in units_keys])\n",
    "        data['dropout_mean'] = np.mean([data[key] for key in dropout_keys])\n",
    "\n",
    "        trial_data.append(data)\n",
    "\n",
    "    df = pd.DataFrame(trial_data)\n",
    "\n",
    "    hp_x = 'learning_rate'\n",
    "    hp_y = 'units_mean'\n",
    "    hp_z = 'dropout_mean'\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    scat = ax.scatter(df[hp_x], df[hp_y], df[hp_z], c=df['score'], cmap='viridis', s=50)\n",
    "    ax.set_xlabel('Learning Rate')\n",
    "    ax.set_ylabel('Média de Units')\n",
    "    ax.set_zlabel('Média de Dropout')\n",
    "    ax.set_title('Visualização 3D dos Hiperparâmetros (Médias)')\n",
    "    fig.colorbar(scat, label='Score de Validação (acurácia)')\n",
    "    plt.show()\n",
    "\n",
    "def plot_hiperparametros_heatmap():\n",
    "  \"\"\"\n",
    "  plota um heatmap com os hiperparâmetros e a acurácia\n",
    "  \"\"\"\n",
    "  trial_data = []\n",
    "\n",
    "  for trial in tuner.oracle.get_best_trials(num_trials=20):\n",
    "        data = trial.hyperparameters.values.copy()\n",
    "        data['score'] = trial.score\n",
    "\n",
    "        units_keys = [key for key in data.keys() if 'units_' in key]\n",
    "        dropout_keys = [key for key in data.keys() if 'dropout_' in key]\n",
    "\n",
    "        data['units_mean'] = np.mean([data[key] for key in units_keys])\n",
    "        data['dropout_mean'] = np.mean([data[key] for key in dropout_keys])\n",
    "\n",
    "        trial_data.append(data)\n",
    "\n",
    "  df = pd.DataFrame(trial_data)\n",
    "\n",
    "  hp1 = 'units_mean'\n",
    "  hp2 = 'dropout_mean'\n",
    "\n",
    "  # Se os hiperparâmetros não existirem, calcule as médias como anteriormente\n",
    "  if 'units_mean' not in df.columns:\n",
    "      units_keys = [key for key in df.columns if 'units_' in key]\n",
    "      df['units_mean'] = df[units_keys].mean(axis=1)\n",
    "\n",
    "  if 'dropout_mean' not in df.columns:\n",
    "      dropout_keys = [key for key in df.columns if 'dropout_' in key]\n",
    "      df['dropout_mean'] = df[dropout_keys].mean(axis=1)\n",
    "\n",
    "  # Criar tabela dinâmica\n",
    "  pivot_table = df.pivot_table(values='score', index=hp1, columns=hp2)\n",
    "\n",
    "  # Plotar heatmap\n",
    "  plt.figure(figsize=(10, 8))\n",
    "  sns.heatmap(pivot_table, annot=True, fmt=\".4f\", cmap='viridis')\n",
    "  plt.title(f'Heatmap de {hp1} vs {hp2}')\n",
    "  plt.xlabel(hp2)\n",
    "  plt.ylabel(hp1)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKlAyjz4f0AI"
   },
   "source": [
    "# Modelagem dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:49:41.989183Z",
     "start_time": "2024-09-09T12:49:32.831855Z"
    },
    "id": "Or7wZVY8OxzB"
   },
   "outputs": [],
   "source": [
    "# adicione caso quera todos os anos\n",
    "# [\n",
    "#     \"./dataset_consumo_2024.csv\",\n",
    "#     \"./dataset_consumo_2023.csv\",\n",
    "#     \"./dataset_consumo_2022.csv\",\n",
    "#     \"./dataset_consumo_2021.csv\",\n",
    "#     \"./dataset_consumo_2019.csv\"\n",
    "# ]\n",
    "\n",
    "\n",
    "arquivos_csv = [\n",
    "    \"./dataset_consumo_2023.csv\"\n",
    "]\n",
    "\n",
    "all_columns_geral = pd.concat([pd.read_csv(arquivo, delimiter=\";\") for arquivo in arquivos_csv], axis=0)\n",
    "fraudes = pd.read_csv('./dataset_fraudes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:49:56.552721Z",
     "start_time": "2024-09-09T12:49:56.383273Z"
    },
    "id": "5zBsStYAf0AI"
   },
   "outputs": [],
   "source": [
    "# troque para True caso queria diminuir do dataset para ficar mais maleável\n",
    "\n",
    "if False: df = reduzir_dataset(all_columns_geral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:53:30.407585Z",
     "start_time": "2024-09-09T12:53:30.374435Z"
    },
    "id": "mw42WzRef0AI"
   },
   "outputs": [],
   "source": [
    "# filtrando pela de 2023\n",
    "\n",
    "fraudes = fraudes[fraudes[\"ANOOS\"] == 2023]\n",
    "fraudes = fraudes.rename(columns={'DESCRICAO_IRREGULARIDADE IDENTIFICADA': 'IRREGULARIDADE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:54:22.640236Z",
     "start_time": "2024-09-09T12:54:22.609887Z"
    },
    "id": "Z3TVW1pCf0AJ"
   },
   "outputs": [],
   "source": [
    "# limitando apenas para a categoria residencial\n",
    "\n",
    "all_columns_geral = all_columns_geral[all_columns_geral[\"CATEGORIA\"].isin([\"RESIDENCIAL\"])]\n",
    "all_columns_geral = all_columns_geral.drop(columns=\"CATEGORIA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:54:27.113790Z",
     "start_time": "2024-09-09T12:54:27.102781Z"
    },
    "id": "3W98-JaOf0AJ"
   },
   "outputs": [],
   "source": [
    "# removendo colunas que não seram utilizandas\n",
    "\n",
    "all_columns_geral = all_columns_geral.drop(columns=['Unnamed: 0',\"FATURADO_MEDIA\", \"SUB_CATEGORIA\", \"STA_TROCA\", \"STA_ACEITA_LEITURA\", 'STA_TROCA', 'EMP_CODIGO', 'COD_GRUPO', 'COD_SETOR_COMERCIAL', 'NUM_QUADRA', 'COD_ROTA_LEITURA', 'SEQ_RESPONSAVEL', 'ECO_RESIDENCIAL', 'ECO_COMERCIAL', 'ECO_INDUSTRIAL', 'ECO_PUBLICA', 'ECO_OUTRAS','LTR_ATUAL', 'LTR_COLETADA', 'DAT_LEITURA', 'DIAS_LEITURA', 'COD_LEITURA_INF_1', 'COD_LEITURA_INF_2', 'COD_LEITURA_INF_3', 'HORA_LEITURA', 'DSC_SIMULTANEA', 'COD_LEITURA_INT','EXCECAO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:54:29.976142Z",
     "start_time": "2024-09-09T12:54:29.958987Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OkNCcAlKf0AJ",
    "outputId": "d7a88026-b60a-45a4-f907-df38bb262ee8"
   },
   "outputs": [],
   "source": [
    "# apagando os valores nulos\n",
    "\n",
    "sem_na = all_columns_geral.dropna()\n",
    "\n",
    "print(f\"quantidade de linhas antes de remover nulos: {len(all_columns_geral)}\")\n",
    "print(f\"quantidade de linhas após de remover nulos: {len(sem_na)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:56:29.283043Z",
     "start_time": "2024-09-09T12:56:29.273621Z"
    },
    "id": "C3Jt1_bJf0AJ"
   },
   "outputs": [],
   "source": [
    "# remove outliers de latitude e longetude\n",
    "\n",
    "def remove_outliers_lat_long(df, column_names):\n",
    "    condition = (df[column_names] > -15).any(axis=1)\n",
    "    df_filtered = df[~condition]\n",
    "    return df_filtered\n",
    "\n",
    "df_sem_linhas = remove_outliers_lat_long(sem_na, ['COD_LATITUDE', 'COD_LONGITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:56:34.700001Z",
     "start_time": "2024-09-09T12:56:30.799047Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "jL95t7Bof0AJ",
    "outputId": "68b5f127-971f-48ef-85bf-5b319302205a"
   },
   "outputs": [],
   "source": [
    "# clusterizando os dados de longetude e latitude\n",
    "\n",
    "kmeans = KMeans(n_clusters=40, random_state=42)\n",
    "df_sem_linhas['cluster'] = kmeans.fit_predict(df_sem_linhas[['COD_LATITUDE', 'COD_LONGITUDE']])\n",
    "\n",
    "# plotando os clusters\n",
    "plotar_clusters(df_sem_linhas, kmeans.n_clusters)\n",
    "\n",
    "# removendo colunas de longetude e latude\n",
    "df_loc_tratado = df_sem_linhas.drop(columns=['COD_LATITUDE', 'COD_LONGITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAve9abrf0AJ"
   },
   "outputs": [],
   "source": [
    "# feature de estimativa de consumo por cluster\n",
    "\n",
    "# TODO:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T12:59:40.049139Z",
     "start_time": "2024-09-09T12:59:40.022476Z"
    },
    "id": "x--EDqQ6f0AJ"
   },
   "outputs": [],
   "source": [
    "# fazendo one hot encoding\n",
    "\n",
    "df_loc_tratado_ohc = pd.get_dummies(df_loc_tratado, columns = ['cluster'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:38:43.116078Z",
     "start_time": "2024-09-09T14:38:43.089272Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7351E37O3eSk",
    "outputId": "58d3339c-cf22-443a-93b3-b00db9d4ea44"
   },
   "outputs": [],
   "source": [
    "df_antes_de_tratar = df_loc_tratado_ohc.copy()\n",
    "df_loc_tratado_ohc[\"TIPO_LIGACAO\"] = df_loc_tratado_ohc[\"TIPO_LIGACAO\"].replace({\"Hidrometrado\": 0, \"Consumo Fixo\": 1})\n",
    "\n",
    "print(f\"antes de tratar: {df_antes_de_tratar['TIPO_LIGACAO'].dtypes}\"); del df_antes_de_tratar\n",
    "print(f\"depois de tratar: {df_loc_tratado_ohc['TIPO_LIGACAO'].dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:34:23.787153Z",
     "start_time": "2024-09-09T14:34:23.771882Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XIBWR5j0f0AJ",
    "outputId": "d5f27ee8-feef-49c8-c8de-f5a469b2ee54"
   },
   "outputs": [],
   "source": [
    "# fazendo a normalização\n",
    "\n",
    "def normalizar(df):\n",
    "    scaler = RobustScaler()\n",
    "    df[['VOLUME_ESTIMADO_ACUM', 'VOLUME_ESTIMADO']] = scaler.fit_transform(df[['VOLUME_ESTIMADO_ACUM', 'VOLUME_ESTIMADO']])\n",
    "    return df\n",
    "\n",
    "print(f\"antes de normalizar:\\n{df_loc_tratado_ohc[['VOLUME_ESTIMADO_ACUM', 'VOLUME_ESTIMADO']].median()}\\n\")\n",
    "df_loc_tratado_ohc = normalizar(df_loc_tratado_ohc)\n",
    "print(f\"depois de normalizar:\\n{df_loc_tratado_ohc[['VOLUME_ESTIMADO_ACUM', 'VOLUME_ESTIMADO']].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:41:35.492705Z",
     "start_time": "2024-09-09T14:41:35.426523Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSquqHp9f0AJ",
    "outputId": "d6254f3d-16bd-461d-8ae8-5077ed4bf6cc"
   },
   "outputs": [],
   "source": [
    "# pivotando a tabela\n",
    "\n",
    "def pivot_table_by_reference(df):\n",
    "    pivoted_df = pd.pivot_table(\n",
    "        df,\n",
    "        index='MATRICULA',\n",
    "        columns='REFERENCIA',\n",
    "        values=['CONS_MEDIDO', 'VOLUME_ESTIMADO'],\n",
    "        aggfunc='sum'\n",
    "    )\n",
    "\n",
    "    pivoted_df.columns = ['_'.join(col).strip() for col in pivoted_df.columns.values]\n",
    "    pivoted_df = pivoted_df.reset_index()\n",
    "\n",
    "    other_columns = df.drop(columns=['CONS_MEDIDO', 'VOLUME_ESTIMADO', 'REFERENCIA']).drop_duplicates()\n",
    "    pivoted_df = pd.merge(pivoted_df, other_columns, on='MATRICULA', how='left')\n",
    "    return pivoted_df.fillna(0)\n",
    "\n",
    "\n",
    "print(f\"antes de pivotar: \\n{df_loc_tratado_ohc.columns}\\n\")\n",
    "pivoted_df = pivot_table_by_reference(df_loc_tratado_ohc)\n",
    "print(f\"depois de pivotar: \\n{pivoted_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:41:35.622683Z",
     "start_time": "2024-09-09T14:41:35.616318Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTHLDus_f0AK",
    "outputId": "f034a3d8-4a8c-4715-9700-678c0d4593b5"
   },
   "outputs": [],
   "source": [
    "# balanciando e simplificando a coluna do tipo de ocorrencia para normal e não normal\n",
    "\n",
    "result = pivoted_df.copy()\n",
    "result['DSC_OCORRENCIA'] = pivoted_df['DSC_OCORRENCIA'].apply(lambda x: 1 if x == 'NORMAL' else 0)\n",
    "\n",
    "print(f\"antes da transformação: {pivoted_df['DSC_OCORRENCIA'].value_counts()}\")\n",
    "print(f\"\\napós da transformação: {result['DSC_OCORRENCIA'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:41:35.921578Z",
     "start_time": "2024-09-09T14:41:35.812920Z"
    },
    "id": "YGGbD0si-Lc8"
   },
   "outputs": [],
   "source": [
    "# removendo outliers\n",
    "\n",
    "def remover_outliers(df, column_name):\n",
    "    # calculando os quartis\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "\n",
    "    # calculando o IQR\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # 3. Determinar os limites superior e inferior\n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "\n",
    "    # filtrar os dados para remover os outliers\n",
    "    df_filtrado = df[(df[column_name] >= limite_inferior) & (df[column_name] <= limite_superior)]\n",
    "\n",
    "    return df_filtrado\n",
    "\n",
    "df_sem_outliers = remover_outliers(result, 'VOLUME_ESTIMADO_ACUM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:41:36.036945Z",
     "start_time": "2024-09-09T14:41:36.024456Z"
    },
    "id": "bKakCwrnAcN3"
   },
   "outputs": [],
   "source": [
    "# tratando a tabela de fraudes\n",
    "\n",
    "dataframe_fraudes_premissa = fraudes[['MATRICULA', 'DESCRICAO']].drop_duplicates()\n",
    "dataframe_fraudes_premissa = pd.get_dummies(dataframe_fraudes_premissa, columns=['DESCRICAO'], dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:41:36.177962Z",
     "start_time": "2024-09-09T14:41:36.170043Z"
    },
    "id": "XoF65SMg_em4"
   },
   "outputs": [],
   "source": [
    "# juntando a tabela de fraudes com a de consumo\n",
    "\n",
    "df_sem_outliers[\"MATRICULA\"] = df_sem_outliers[\"MATRICULA\"].astype(int)\n",
    "\n",
    "dataframe_pf_premissa = pd.merge(df_sem_outliers, dataframe_fraudes_premissa, on='MATRICULA', how='left')\n",
    "\n",
    "dataframe_pf_premissa = dataframe_pf_premissa.rename(columns={\"DESCRICAO_IRREGULARIDADE IDENTIFICADA\": \"IRREGULARIDADE\"})\n",
    "\n",
    "dataframe_pf_premissa['IRREGULARIDADE'] = dataframe_pf_premissa['IRREGULARIDADE'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:41:36.782091Z",
     "start_time": "2024-09-09T14:41:36.770998Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "Iuonqjz0f0AK",
    "outputId": "ee441a52-42d0-4fee-bbb2-bb8c4779e818"
   },
   "outputs": [],
   "source": [
    "dataframe_pf_premissa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:41:37.140676Z",
     "start_time": "2024-09-09T14:41:37.116937Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdvEDf7-CoYp",
    "outputId": "656b8ac0-c1c6-4768-8079-3dee9150a8fc"
   },
   "outputs": [],
   "source": [
    "# balanciamento os dados\n",
    "\n",
    "def balanciar(df):\n",
    "  smote = SMOTE(random_state=42)\n",
    "  X = df.drop('IRREGULARIDADE', axis=1)\n",
    "  y = df['IRREGULARIDADE']\n",
    "  X_res, y_res = smote.fit_resample(X, y)\n",
    "  return pd.concat([X_res, y_res], axis=1)\n",
    "\n",
    "verificar_quantidade = lambda df: np.unique(df['IRREGULARIDADE'].values, return_counts=True)\n",
    "\n",
    "print(f\"antes de balanciar: {dataframe_pf_premissa['IRREGULARIDADE'].unique}\")\n",
    "dataframe_pf_premissa = balanciar(dataframe_pf_premissa)\n",
    "print(f\"depois de balanciar: {dataframe_pf_premissa['IRREGULARIDADE'].unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0rS6VWQf0AK"
   },
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:41:49.776747Z",
     "start_time": "2024-09-09T14:41:49.768282Z"
    },
    "id": "VN4ph0X3zJGc"
   },
   "outputs": [],
   "source": [
    "# dividindo as features das targuets\n",
    "\n",
    "y = dataframe_pf_premissa['IRREGULARIDADE'].values\n",
    "X = dataframe_pf_premissa.drop(['MATRICULA','IRREGULARIDADE'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T14:41:50.159484Z",
     "start_time": "2024-09-09T14:41:50.152975Z"
    },
    "id": "ZSYszgHggtMX"
   },
   "outputs": [],
   "source": [
    "# fazendo a divisão de treino e teste\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm77G3AkgkKs"
   },
   "outputs": [],
   "source": [
    "# Definindo a função para construir o modelo\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=(X_train.shape[1],)))\n",
    "\n",
    "    # Hyperparameters para o número de unidades nas camadas\n",
    "    for i in range(hp.Int('num_layers', 2, 4)):  # entre 2 e 4 camadas densas\n",
    "        model.add(keras.layers.Dense(units=hp.Int(f'units_{i}', min_value=64, max_value=512, step=64), activation='relu'))\n",
    "        model.add(keras.layers.Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Hyperparameters para o otimizador e a taxa de aprendizado\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Definindo o tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir',\n",
    "    project_name='keras_tuning')\n",
    "\n",
    "# Iniciando o grid search\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aAUyh8056hY8",
    "outputId": "807d1199-7106-4279-c016-ccbd11bd48c7"
   },
   "outputs": [],
   "source": [
    "for i, model in enumerate(best_models):\n",
    "    print(f\"\\nArquitetura do Modelo n{i+1}:\")\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fazer teste com o melhor modelo para criar métricas\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "y_pred_prob = best_model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matriz de confusão\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plotar_matrix_de_consusao(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa matriz de confusão, podemos ver bons resultados, com uma quantidade relativamente baixa de falsos negativos (25.992) e falsos positivos (8.328). Nossa principal métrica, sendo a precisão, reflete o quão bem o modelo conseguiu classificar corretamente as instâncias positivas. Com um número elevado de verdadeiros positivos (75.376) e poucos falsos positivos, podemos dizer que a precisão do modelo é boa. Dessa forma, podemos inferir que o modelo encontrado através do random search foi eficaz e, no final, conseguimos obter um bom desempenho preditivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_hiperparametros_2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse gráfico 2D, onde podemos observar a correlação entre os resultados dos modelos com base no learning rate e no tamanho do modelo, podemos inferir que os modelos com melhor desempenho foram aqueles que utilizaram uma taxa de aprendizado menor. Além disso, os modelos com tamanhos variando de 250 a 450 apresentaram as melhores performances, sugerindo que essa combinação de parâmetros foi mais eficiente em ajustar o modelo e alcançar bons resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotar_hiperparametros_3d(tuner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste gráfico 3D, podemos visualizar a correlação entre o tamanho do modelo, a taxa de aprendizado e a quantidade de dropout, formando uma análise em três dimensões. Podemos inferir que os modelos que performaram melhor foram aqueles com uma taxa de aprendizado menor, com tamanhos moderados, e uma taxa de dropout variando entre 0,2 e 0,38. Essa combinação parece ter contribuído para uma melhor generalização dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hiperparametros_heatmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse heatmap, podemos ver a correlação entre o tamanho do modelo e a média de dropout, além da média do tamanho do modelo. Podemos observar que os modelos que performaram melhor foram aqueles com uma média de tamanho de 277 ou menos. Esses modelos, combinados com uma média de dropout adequada, apresentaram os melhores resultados, sugerindo que tamanhos menores e valores de dropout moderados levaram a um melhor desempenho."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KKlAyjz4f0AI"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
